{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b48c4b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e8ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac6d8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.13.1\n",
      "IPython version      : 8.30.0\n",
      "\n",
      "torch       : 2.6.0+cu118\n",
      "numpy       : 2.2.2\n",
      "pandas      : 2.2.3\n",
      "scikit-learn: 1.6.0\n",
      "seaborn     : 0.13.2\n",
      "matplotlib  : 3.9.3\n",
      "tensorflow  : not installed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from watermark import watermark\n",
    "\n",
    "# Printing versions of libraries used\n",
    "%load_ext watermark\n",
    "%watermark -v -p torch,numpy,pandas,scikit-learn,seaborn,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90cf9de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c594629a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/beweird/text-for-next-word-predictor?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.13M/1.13M [00:01<00:00, 752kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset text file: C:\\Users\\borat\\.cache\\kagglehub\\datasets\\beweird\\text-for-next-word-predictor\\versions\\1\\leo tolstoy - war and peace.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Downloading latest version\n",
    "path = kagglehub.dataset_download(\"beweird/text-for-next-word-predictor\")\n",
    "file_path = os.path.join(path, \"leo tolstoy - war and peace.txt\")\n",
    "print(\"Path to dataset text file:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of lines:  30588\n",
      "Total unique words:  17877\n"
     ]
    }
   ],
   "source": [
    "# Open and read the contents of the file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "filtered_text = re.sub(r'-', ' ', text)\n",
    "filtered_text = re.sub('[^a-zA-Z0-9 \\.\\n]', '', filtered_text)\n",
    "filtered_text = filtered_text.lower()\n",
    "\n",
    "lines=filtered_text.split(\".\")\n",
    "words=['.']\n",
    "for l in lines:\n",
    "    for w in l.split():\n",
    "        if (len(w)>0):\n",
    "            words.append(w)\n",
    "words=list(pd.Series(words).unique())\n",
    "\n",
    "\n",
    "print(\"Total no. of lines: \", len(lines))\n",
    "print(\"Total unique words: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5685eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 words:  ['well prince so genoa and lucca are now just family estates of the buonapartes', ' but i warn you if you dont tell me that this means war if you still try to defend the infamies and horrors perpetrated by that antichrist  i really believe he is antichrist  i will have nothing more to do with you and you are no longer my friend no longer my faithful slave as you call yourself but how do you do i see i have frightened you  sit down and tell me all the news']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37683d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17877\n"
     ]
    }
   ],
   "source": [
    "stoi={s:i for i,s in enumerate(words)}\n",
    "itos={i:s for s,i in stoi.items()}\n",
    "print(len(itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e620f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([592621, 5])\n",
      "Y shape:  torch.Size([592621])\n"
     ]
    }
   ],
   "source": [
    "block_size = 5 # how many characters we take to predict the next character\n",
    "X = []\n",
    "Y = []\n",
    "for l in lines:\n",
    "    context = [0] * block_size\n",
    "    words = l.split()\n",
    "\n",
    "    if len(words) == 0:\n",
    "        continue\n",
    "\n",
    "    for w in words:\n",
    "        ix = stoi[w]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "        \n",
    "\n",
    "    ix = stoi['.']\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long, device=device)\n",
    "Y = torch.tensor(Y, dtype=torch.long, device=device)\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"Y shape: \", Y.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "519c0516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(17877, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emb_dim = 64 # Hyperparameter\n",
    "\n",
    "# Embedding layer\n",
    "emb=torch.nn.Embedding(len(stoi),emb_dim).to(device)\n",
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95a96487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Next_Word_Predictor(nn.Module):\n",
    "    def __init__(self, block_size, vocab_size, emb_dim, hidden_dim, activation_fn, seed_value):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.hyperparams = {'block_size':self.block_size, 'emb_dim':emb_dim, 'hidden_dim':hidden_dim, 'activation_fn':activation_fn, 'seed_value':seed_value}\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.linear1 = nn.Linear(block_size * emb_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "         \n",
    "        if activation_fn == 'sigmoid':\n",
    "            self.activation = torch.sigmoid  \n",
    "        else:\n",
    "            self.activation = torch.relu \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.shape[0], -1)  \n",
    "        \n",
    "        # Hidden layer\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f545959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, block_size, emb_dim, hidden_dim, activation_fn, seed_value, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    model = Next_Word_Predictor(block_size, len(stoi), emb_dim, hidden_dim, activation_fn, seed_value).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            x_batch = X[i:i+batch_size].to(device)\n",
    "            y_batch = Y[i:i+batch_size].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/((len(X)-1)//batch_size + 1):.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "82a83091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 6.0911\n",
      "Epoch [2/100], Loss: 5.1650\n",
      "Epoch [3/100], Loss: 4.4374\n",
      "Epoch [4/100], Loss: 3.8284\n",
      "Epoch [5/100], Loss: 3.4647\n",
      "Epoch [6/100], Loss: 3.2028\n",
      "Epoch [7/100], Loss: 2.9985\n",
      "Epoch [8/100], Loss: 2.8334\n",
      "Epoch [9/100], Loss: 2.6965\n",
      "Epoch [10/100], Loss: 2.5804\n",
      "Epoch [11/100], Loss: 2.4803\n",
      "Epoch [12/100], Loss: 2.3925\n",
      "Epoch [13/100], Loss: 2.3144\n",
      "Epoch [14/100], Loss: 2.2438\n",
      "Epoch [15/100], Loss: 2.1798\n",
      "Epoch [16/100], Loss: 2.1210\n",
      "Epoch [17/100], Loss: 2.0669\n",
      "Epoch [18/100], Loss: 2.0165\n",
      "Epoch [19/100], Loss: 1.9694\n",
      "Epoch [20/100], Loss: 1.9252\n",
      "Epoch [21/100], Loss: 1.8837\n",
      "Epoch [22/100], Loss: 1.8442\n",
      "Epoch [23/100], Loss: 1.8074\n",
      "Epoch [24/100], Loss: 1.7720\n",
      "Epoch [25/100], Loss: 1.7383\n",
      "Epoch [26/100], Loss: 1.7061\n",
      "Epoch [27/100], Loss: 1.6754\n",
      "Epoch [28/100], Loss: 1.6460\n",
      "Epoch [29/100], Loss: 1.6181\n",
      "Epoch [30/100], Loss: 1.5910\n",
      "Epoch [31/100], Loss: 1.5651\n",
      "Epoch [32/100], Loss: 1.5399\n",
      "Epoch [33/100], Loss: 1.5159\n",
      "Epoch [34/100], Loss: 1.4927\n",
      "Epoch [35/100], Loss: 1.4707\n",
      "Epoch [36/100], Loss: 1.4493\n",
      "Epoch [37/100], Loss: 1.4286\n",
      "Epoch [38/100], Loss: 1.4088\n",
      "Epoch [39/100], Loss: 1.3895\n",
      "Epoch [40/100], Loss: 1.3705\n",
      "Epoch [41/100], Loss: 1.3528\n",
      "Epoch [42/100], Loss: 1.3353\n",
      "Epoch [43/100], Loss: 1.3189\n",
      "Epoch [44/100], Loss: 1.3025\n",
      "Epoch [45/100], Loss: 1.2866\n",
      "Epoch [46/100], Loss: 1.2712\n",
      "Epoch [47/100], Loss: 1.2571\n",
      "Epoch [48/100], Loss: 1.2423\n",
      "Epoch [49/100], Loss: 1.2287\n",
      "Epoch [50/100], Loss: 1.2151\n",
      "Epoch [51/100], Loss: 1.2024\n",
      "Epoch [52/100], Loss: 1.1892\n",
      "Epoch [53/100], Loss: 1.1771\n",
      "Epoch [54/100], Loss: 1.1650\n",
      "Epoch [55/100], Loss: 1.1535\n",
      "Epoch [56/100], Loss: 1.1414\n",
      "Epoch [57/100], Loss: 1.1303\n",
      "Epoch [58/100], Loss: 1.1192\n",
      "Epoch [59/100], Loss: 1.1086\n",
      "Epoch [60/100], Loss: 1.0982\n",
      "Epoch [61/100], Loss: 1.0884\n",
      "Epoch [62/100], Loss: 1.0781\n",
      "Epoch [63/100], Loss: 1.0685\n",
      "Epoch [64/100], Loss: 1.0597\n",
      "Epoch [65/100], Loss: 1.0508\n",
      "Epoch [66/100], Loss: 1.0420\n",
      "Epoch [67/100], Loss: 1.0332\n",
      "Epoch [68/100], Loss: 1.0249\n",
      "Epoch [69/100], Loss: 1.0165\n",
      "Epoch [70/100], Loss: 1.0090\n",
      "Epoch [71/100], Loss: 1.0013\n",
      "Epoch [72/100], Loss: 0.9932\n",
      "Epoch [73/100], Loss: 0.9862\n",
      "Epoch [74/100], Loss: 0.9797\n",
      "Epoch [75/100], Loss: 0.9725\n",
      "Epoch [76/100], Loss: 0.9655\n",
      "Epoch [77/100], Loss: 0.9586\n",
      "Epoch [78/100], Loss: 0.9525\n",
      "Epoch [79/100], Loss: 0.9464\n",
      "Epoch [80/100], Loss: 0.9402\n",
      "Epoch [81/100], Loss: 0.9342\n",
      "Epoch [82/100], Loss: 0.9283\n",
      "Epoch [83/100], Loss: 0.9228\n",
      "Epoch [84/100], Loss: 0.9180\n",
      "Epoch [85/100], Loss: 0.9128\n",
      "Epoch [86/100], Loss: 0.9061\n",
      "Epoch [87/100], Loss: 0.9005\n",
      "Epoch [88/100], Loss: 0.8954\n",
      "Epoch [89/100], Loss: 0.8900\n",
      "Epoch [90/100], Loss: 0.8852\n",
      "Epoch [91/100], Loss: 0.8801\n",
      "Epoch [92/100], Loss: 0.8758\n",
      "Epoch [93/100], Loss: 0.8710\n",
      "Epoch [94/100], Loss: 0.8667\n",
      "Epoch [95/100], Loss: 0.8616\n",
      "Epoch [96/100], Loss: 0.8578\n",
      "Epoch [97/100], Loss: 0.8528\n",
      "Epoch [98/100], Loss: 0.8493\n",
      "Epoch [99/100], Loss: 0.8443\n",
      "Epoch [100/100], Loss: 0.8407\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(stoi)\n",
    "hidden_dim = 1024\n",
    "activation_fn = 'relu'\n",
    "seed_value = 42\n",
    "model = train(X, Y, block_size, emb_dim, hidden_dim, activation_fn, seed_value, epochs=100, batch_size=1024, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "431dd449",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_variant_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6289762d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Next_Word_Predictor(\n",
       "  (emb): Embedding(17877, 64)\n",
       "  (linear1): Linear(in_features=320, out_features=1024, bias=True)\n",
       "  (linear2): Linear(in_features=1024, out_features=17877, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model with weights_only=False\n",
    "model_1 = torch.load('model_variant_1.pth', map_location=device, weights_only=False)\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff2cdfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate names from trained model\n",
    "\n",
    "def generate_next_words(model, itos, stoi, content, seed_value, k, temperature=1.0, max_len=10):\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    block_size = model.block_size\n",
    "    context = content.lower()\n",
    "    context = re.sub('[^a-zA-Z0-9 \\.]', '', context)\n",
    "    context = re.sub('\\.', ' . ', context)\n",
    "    word_c = context.split()\n",
    "    context = []\n",
    "    for i in range(len(word_c)):\n",
    "        try:\n",
    "            if stoi[word_c[i]]:\n",
    "                context.append(word_c[i])\n",
    "        except:\n",
    "            context = [stoi[w] for w in context]\n",
    "            if len(context) <= block_size:\n",
    "                context = [0] * (block_size - len(context)) + context\n",
    "            elif len(context) > block_size:\n",
    "                context = context[-block_size:]\n",
    "            x = torch.tensor(context).view(1, -1).to(device)\n",
    "            y_pred = model(x)\n",
    "            logits = y_pred\n",
    "            logits = logits/temperature\n",
    "\n",
    "            ix = torch.distributions.categorical.Categorical(logits=logits).sample().item()\n",
    "            word = itos[ix]\n",
    "            content += \" \" + word\n",
    "            context = context [1:] + [ix]\n",
    "            context = [itos[w] for w in context]\n",
    "            \n",
    "    context = [stoi[w] for w in context]\n",
    "               \n",
    "    if len(context) <= block_size:\n",
    "        context = [0] * (block_size - len(context)) + context\n",
    "    elif len(context) > block_size:\n",
    "        context = context[-block_size:]\n",
    "\n",
    "    for i in range(k):\n",
    "        x = torch.tensor(context).view(1, -1).to(device)\n",
    "        y_pred = model(x)\n",
    "        logits = y_pred\n",
    "        logits = logits/temperature\n",
    "        ix = torch.distributions.categorical.Categorical(logits=logits).sample().item()\n",
    "        word = itos[ix]\n",
    "        content += \" \" + word\n",
    "        context = context [1:] + [ix]\n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5054a3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "and he quickly turned around and just to realize that his friend is no longer nothing of nicholas circumstances as if anything are in that power of life is an\n"
     ]
    }
   ],
   "source": [
    "para =\"\"\n",
    "content = input(\"Enter a paragraph to start the text generation: \")\n",
    "k = int(input(\"Enter the number of next words to predict (k): \"))\n",
    "\n",
    "generated_text = generate_next_words(model_1, itos, stoi, content, seed_value, k, temperature = 1.0)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
